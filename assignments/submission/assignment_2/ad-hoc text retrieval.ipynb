{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings for ad-hoctext retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "import re\n",
    "# Use anserini and run java code\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = '/Library/Java/JavaVirtualMachines/jdk1.8.0_161.jdk/Contents/Home'\n",
    "\n",
    "import jnius_config\n",
    "jnius_config.set_classpath(\"/Users/silvan/ucph/Block4-19/IR2019/assignments/assignment_2/anserini/target/anserini-0.5.1-SNAPSHOT-fatjar.jar\")\n",
    "\n",
    "from jnius import autoclass\n",
    "JString = autoclass('java.lang.String')\n",
    "JSearcher = autoclass('io.anserini.search.SimpleSearcher')\n",
    "\n",
    "searcher = JSearcher(JString('/Users/silvan/ucph/Block4-19/IR2019/assignments/assignment_2/anserini/lucene-index.robust04.pos+docvectors+rawdocs'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_queries = []\n",
    "with open(\"data/04.testset\") as f:\n",
    "    lines = f.readlines()\n",
    "line_num = 0    \n",
    "for line in lines:\n",
    "    line_num += 1\n",
    "    if line.startswith('<num> Number:'):\n",
    "        currentId = line.replace('<num> Number:', '').strip()\n",
    "    if line.startswith('<title>'):\n",
    "        text = line.replace('<title>', '').strip()\n",
    "        if not text:\n",
    "            text = lines[line_num]\n",
    "        if currentId is not None and text is not None:\n",
    "            text_queries.append((currentId,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "train_queries = text_queries[:int(0.8 * len(text_queries))]\n",
    "test_queries = text_queries[int(0.8 * len(text_queries)):]\n",
    "print(len(train_queries))\n",
    "print(len(test_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "40\n",
      "40\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "fold1, fold2, fold3, fold4, fold5 = np.array_split(train_queries,5)\n",
    "folds = [fold1, fold2, fold3, fold4, fold5]\n",
    "print(len(fold1))\n",
    "print(len(fold2))\n",
    "print(len(fold3))\n",
    "print(len(fold4))\n",
    "print(len(fold5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('301', 'International Organized Crime')\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "print(text_queries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use anserini to get BM25 rankings\n",
    "def get_ranking(query_text,run_length=1000,k1=1.5, b=0.75):\n",
    "    searcher.setBM25Similarity(k1,b)\n",
    "    hits = searcher.search(JString(query_text),run_length)\n",
    "    scores, docids = [h.score for h in hits], [h.docid for h in hits]\n",
    "    return (scores, docids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranking_all(queries,output_file,k1=1.5, b=0.75,fixed='Q0',run='BM25',run_length=100):\n",
    "    all_data = pd.DataFrame()\n",
    "    topic_list = []\n",
    "    for query in queries:\n",
    "        topic, query_text = query\n",
    "        doc_scores, doc_ids = get_ranking(query_text,run_length,k1=k1,b=b)\n",
    "\n",
    "        hitslen = len(doc_ids)\n",
    "        qids, ranks = [topic] * hitslen, np.arange(1,hitslen+1), \n",
    "        runs, fixeds = [run] * hitslen, [fixed] * hitslen\n",
    "        # found scores and ids to dataframe\n",
    "        topic_df = pd.DataFrame({'docid' : doc_ids,'score' : doc_scores})\n",
    "        # runs need format: topic fixed docid rank score run\n",
    "        topic_df.insert(0, 'topic', topic)\n",
    "        topic_df.insert(1, 'fixed', 'Q0')\n",
    "        topic_df.reset_index()\n",
    "        topic_df.insert(3, 'rank', topic_df.index+1)\n",
    "        topic_df.insert(5, 'run', run)\n",
    "        topic = topic_df.drop_duplicates(subset='docid', keep='first')\n",
    "        topic_list.append(topic)\n",
    "    all_data = pd.concat(topic_list)\n",
    "    all_data.to_csv(output_file, header=None, index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trec_eval(qrels,run_output):\n",
    "    trec = \"trec_eval.9.0/trec_eval\"\n",
    "    cli_output = subprocess.run([trec, '-q', '-m', 'all_trec', qrels, run_output], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    cli_output=cli_output.split(\"\\n\")\n",
    "    map_values = []\n",
    "    for line in cli_output:\n",
    "        line = line.split('\\t')\n",
    "        try:\n",
    "            map_outputs = ['map','map_cut_5','map_cut_10','map_cut_20']\n",
    "            if line[0].strip() in map_outputs and line[1] == 'all':\n",
    "                map_values.append((line[0],line[2]))\n",
    "        except:\n",
    "            continue\n",
    "    return map_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = \"data/qrels.robust2004.txt\"\n",
    "run = \"data/run.bm25.txt\"\n",
    "k1 = [0.5,1,1.5,2.0]\n",
    "b = [0.25,0.5,0.75,1]\n",
    "\n",
    "parameters = []\n",
    "for i in range(len(k1)):\n",
    "    for o in range(len(b)):\n",
    "        for n in range(5):\n",
    "            get_ranking_all(queries=folds[n],output_file=run,k1=k1[i],b=b[o])\n",
    "            parameters.append((float(trec_eval(qrels,run)[0][1]),k1[i],b[o]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('map                   ', '0.2326'),\n",
       " ('map_cut_5             ', '0.0914'),\n",
       " ('map_cut_10            ', '0.1358'),\n",
       " ('map_cut_20            ', '0.1773')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_parameters = sorted(parameters, key=lambda tup: tup[0], reverse=True)\n",
    "_, k1, b = sorted_parameters[0]\n",
    "get_ranking_all(queries=test_queries,output_file=run,k1=k1,b=b)\n",
    "# Eval with trac\n",
    "trec_eval(qrels,run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Query BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid aproach (taking average)\n",
    "def qe_centroid_method(queries,n):\n",
    "    expanded_queries_centroid = []\n",
    "    for query in queries:\n",
    "        topic, query_text = query\n",
    "        query_terms = query_text.lower().split()\n",
    "        vectors = np.zeros(300) # dim 300\n",
    "        for word in query_terms:\n",
    "            if word in model.vocab:\n",
    "                vectors += model[word]\n",
    "        similar_words = model.similar_by_vector(vectors,topn=n)\n",
    "        expansion = \" \".join([ x[0] for x in similar_words ])\n",
    "        query_terms = \" \".join(query_terms)\n",
    "        expanded_query = query_terms + \" \" + expansion\n",
    "        expanded_queries_centroid.append((topic,expanded_query))\n",
    "    return expanded_queries_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 'data/run.expanded.centroid.bm25.txt'\n",
    "n = [1,2,3,4]\n",
    "parameters = []\n",
    "for i in range(len(n)):\n",
    "    for o in range(len(folds)):\n",
    "        fold_queries = qe_centroid_method(folds[o],n[i])\n",
    "        get_ranking_all(queries=fold_queries,output_file=run,run=\"BM25+QE\",k1=k1,b=b)\n",
    "        parameters.append((float(trec_eval(qrels,run)[0][1]),n[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_parameters = sorted(parameters, key=lambda tup: tup[0], reverse=True)\n",
    "_, n = sorted_parameters[0]\n",
    "exp_queries_cent = qe_centroid_method(text_queries,n)\n",
    "get_ranking_all(queries=exp_queries_cent,output_file=run,run=\"BM25+QE\",k1=k1,b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('map                   ', '0.1832'),\n",
       " ('map_cut_5             ', '0.0751'),\n",
       " ('map_cut_10            ', '0.1089'),\n",
       " ('map_cut_20            ', '0.1408')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = \"data/qrels.robust2004.txt\"\n",
    "trec_eval(qrels,run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(similar_words):\n",
    "    exps = [(i,np.exp(v)) for i,v in similar_words]\n",
    "    exps_sum = [np.exp(v) for i,v in similar_words]\n",
    "    sum_of_exps = sum(exps_sum)\n",
    "    return [(i,v/sum_of_exps) for i,v in exps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion aproach\n",
    "def fusion_method(queries,n,v):\n",
    "    expanded_queries_fusion = []\n",
    "    for query in queries:\n",
    "        query_dict = {} # get rid of double words, set highest one\n",
    "        topic, query_text = query\n",
    "        query_terms = query_text.lower().split()\n",
    "        expanding_words = []\n",
    "        for word in query_terms:\n",
    "            if word in model.vocab:\n",
    "                expanding_words.append(model.similar_by_word(word,topn=n))\n",
    "        for word in expanding_words:\n",
    "            for word,sim in word:\n",
    "                try:\n",
    "                    if query_dict[word] < sim:\n",
    "                        query_dict[word] = sim\n",
    "                except:\n",
    "                    query_dict[word] = sim \n",
    "        \n",
    "        sorted_words = sorted(softmax(query_dict.items()),key=lambda x: x[1], reverse=True)[:v]\n",
    "        expansion = \" \".join([ x[0] for x in sorted_words ])\n",
    "        expanded_queries_fusion.append((topic, query_text + \" \" + expansion.replace(\"ñ\", \"n\"))) # quick fix Spanish character\n",
    "\n",
    "    return expanded_queries_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [1,3]\n",
    "v = [1,3]\n",
    "run = 'data/expanded.fusion.queries.txt'\n",
    "parameters = []\n",
    "for p in range(len(folds)):\n",
    "    for i in range(len(n)):\n",
    "        for o in range(len(v)):\n",
    "            fold_fus = fusion_method(folds[p],n[i],v[o])\n",
    "            get_ranking_all(queries=fold_fus,output_file=run,run=\"BM25+QE\")\n",
    "            parameters.append((float(trec_eval(qrels,run)[0][1]),n[i],v[o]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_parameters = sorted(parameters, key=lambda tup: tup[0], reverse=True)\n",
    "_, n, v = sorted_parameters[0]\n",
    "exp_queries_fus = fusion_method(test_queries,n,v)\n",
    "get_ranking_all(queries=exp_queries_fus,output_file=run,run=\"BM25+QE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('map                   ', '0.1903'),\n",
       " ('map_cut_5             ', '0.0778'),\n",
       " ('map_cut_10            ', '0.1100'),\n",
       " ('map_cut_20            ', '0.1420')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = \"data/qrels.robust2004.txt\"\n",
    "trec_eval(qrels,run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text needs to be split in single words, remove unneeded texts\n",
    "REPLACE_BY_SPACE = re.compile('[/(){}\\[\\]|@,;]')\n",
    "BAD_SYMBOLS = re.compile('[^a-z #+_]')\n",
    "TAGS = re.compile('<[^<]+?>')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(content):\n",
    "    content = content.lower()\n",
    "    content = REPLACE_BY_SPACE.sub(' ', content)\n",
    "    content = BAD_SYMBOLS.sub(' ', content)\n",
    "    content = TAGS.sub('', content)\n",
    "    content = ' '.join(word for word in content.split() if word not in STOPWORDS)\n",
    "    remove_more_words = ['bfn','b','f'] # not in word2vec vocab and also not needed\n",
    "    content = ' '.join(word for word in content.split() if word not in remove_more_words)\n",
    "    content = content.split()\n",
    "    return content\n",
    "\n",
    "def get_ranking_with_content(query,run_length=2):\n",
    "    _, query_text = query\n",
    "    hits = searcher.search(JString(query_text), run_length)\n",
    "    topic_content, doc_ids = [h.content for h in hits], [h.docid for h in hits] \n",
    "    topic_content_df = pd.DataFrame({'content': topic_content, 'docid': doc_ids})\n",
    "    topic_content_df['content'] = topic_content_df['content'].apply(preprocess_text)\n",
    "    return topic_content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_we(queries,output_file,k1=1.5,b=0.75,run='BM25+WE',run_length=100):\n",
    "    topic_list = []\n",
    "    for query in queries:\n",
    "        topic, query_text = query\n",
    "        query_terms = query_text.lower().split()\n",
    "        bm25_ranking = get_ranking_with_content(query,run_length=run_length)\n",
    "        docids = bm25_ranking['docid'].values\n",
    "        content = bm25_ranking['content'].values\n",
    "        word_corpus = Dictionary(content)\n",
    "        \n",
    "        # average sentence length\n",
    "        avgsl = int(np.mean(np.array([len(x) for x in content])))\n",
    "\n",
    "        scores = []\n",
    "        for document in content:\n",
    "            score = 0\n",
    "            # loop through longer text, that we don't overlook any term\n",
    "            for word in document:\n",
    "                sem = 0\n",
    "                sim_list = []\n",
    "                for term in query_terms:\n",
    "                    # word might not be in vocab, so similarity = 0\n",
    "                    try:\n",
    "                        # cosine similarity\n",
    "                        sim = model.similarity(word,term)\n",
    "                    except:\n",
    "                        sim = 0\n",
    "                    sim_list.append(sim)\n",
    "                \n",
    "                sem = max(sim_list)\n",
    "                \n",
    "                token = word_corpus.token2id[word]\n",
    "                doc_fqs = word_corpus.dfs[token]\n",
    "                \n",
    "                idf = np.log(word_corpus.num_docs/doc_fqs)\n",
    "                top_frac = sem * (k1 + 1)\n",
    "                text_frac = len(query_terms)/avgsl\n",
    "                lower_frac = sem + k1 * (1-b+b*text_frac)\n",
    "                score += idf * top_frac/lower_frac\n",
    "            scores.append(score)\n",
    "        topic_df = pd.DataFrame({'docid': docids,'score': scores}).sort_values(by=['score'], ascending=False)\n",
    "        # runs need format: topic fixed docid rank score run\n",
    "        topic_df.insert(0, 'topic', topic)\n",
    "        topic_df.insert(1, 'fixed', 'Q0')\n",
    "        topic_df.reset_index()\n",
    "        topic_df.insert(3, 'rank', topic_df.index+1)\n",
    "        topic_df.insert(5, 'run', run)\n",
    "        topic_df = topic_df.drop_duplicates(subset='docid', keep='first')\n",
    "        topic_list.append(topic_df)\n",
    "    \n",
    "    all_data = pd.concat(topic_list)\n",
    "    all_data.to_csv(output_file, header=None, index=None, sep=' ', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = [1,1.5,2]\n",
    "b = [0.5,0.75,1]\n",
    "parameters = []\n",
    "for p in range(len(folds)):\n",
    "    for i in range(len(k1)):\n",
    "        for o in range(len(b)):\n",
    "            fold_we = bm25_we(folds[p],'data/run.we.bm25.txt',k1[i],b[o])\n",
    "            parameters.append((float(trec_eval(qrels,'data/run.we.bm25.txt')[0][1]),k1[i],b[o]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_parameters = sorted(parameters, key=lambda tup: tup[0], reverse=True)\n",
    "_, k1, b = sorted_parameters[0]\n",
    "bm25_we(test_queries,'data/run.we.bm25.txt',k1,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('map                   ', '0.1458'),\n",
       " ('map_cut_5             ', '0.0414'),\n",
       " ('map_cut_10            ', '0.0577'),\n",
       " ('map_cut_20            ', '0.0897')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trec_eval(qrels,'data/run.we.bm25.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
