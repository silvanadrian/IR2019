{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings for ad-hoctext retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Queries + Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = dict()\n",
    "   \n",
    "corpus = open (\"data/trec_corpus.txt\", \"r\")\n",
    "    \n",
    "for line in corpus.readlines():\n",
    "    splitted_line = line.split(\" \")\n",
    "    text = splitted_line[1:]\n",
    "    docid = splitted_line[0]\n",
    "    text_corpus[docid] = text\n",
    "\n",
    "del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drlat', 'o', 'fbis', 'lat', 'document', 'type', 'daily', 'report', 'mar', 'colombia', 'criticizes', 'drug', 'consuming', 'countries', 'pa', 'santa', 'fe', 'de', 'bogota', 'inravision', 'television', 'cadena', 'in', 'spanish', 'gmt', 'mar', 'pa', 'santa', 'fe', 'de', 'bogota', 'inravision', 'television', 'cadena', 'language', 'spanish', 'article', 'type', 'bfn', 'from', 'the', 'ntc', 'news', 'newscast', 'text', 'faced', 'with', 'criticism', 'triggered', 'by', 'the', 'cali', 'cartel', 'surrender', 'process', 'the', 'prosecutor', 'general', 's', 'office', 'has', 'issued', 'a', 'communique', 'the', 'communique', 'from', 'the', 'prosecutor', 'general', 'contains', 'seven', 'specific', 'points', 'his', 'activities', 'have', 'been', 'directed', 'at', 'the', 'fight', 'against', 'all', 'manifestations', 'of', 'crime', 'the', 'destruction', 'of', 'the', 'medellin', 'cartel', 'was', 'due', 'to', 'constant', 'work', 'by', 'the', 'prosecutor', 'general', 's', 'office', 'number', 'as', 'heard', 'all', 'the', 'work', 'of', 'the', 'prosecutor', 'general', 's', 'office', 'has', 'been', 'carried', 'out', 'within', 'the', 'framework', 'of', 'the', 'penal', 'code', 'and', 'the', 'law', 'recently', 'reformed', 'by', 'congress', 'with', 'full', 'knowledge', 'of', 'the', 'government', 'the', 'specific', 'objective', 'of', 'the', 'criticism', 'regarding', 'the', 'spiritless', 'contribution', 'of', 'consumer', 'countries', 'to', 'the', 'fight', 'against', 'drug', 'trafficking', 'is', 'to', 'fight', 'that', 'illegal', 'activity', 'the', 'final', 'point', 'of', 'the', 'communique', 'notes', 'the', 'legalization', 'of', 'some', 'drugs', 'could', 'become', 'a', 'deadly', 'weapon', 'capable', 'of', 'depriving', 'drug', 'traffickers', 'of', 'profits', 'the', 'prosecutor', 'general', 'concluded', 'the', 'communique', 'by', 'stating', 'he', 'would', 'rather', 'die', 'than', 'live', 'in', 'a', 'country', 'where', 'criminals', 'can', 'decide', 'who', 'will', 'be', 'the', 'country', 's', 'government', 'officials', 'and', 'judges\\n']\n"
     ]
    }
   ],
   "source": [
    "# Example document\n",
    "print(text_corpus[\"FBIS3-11318\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_queries = []\n",
    "with open(\"data/queries.txt\") as f:\n",
    "    lines = ''.join(f.readlines())\n",
    "text_queries = [line.rstrip().split() for line in lines.split('\\n')[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['301', 'international', 'organized', 'crime']\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "print(text_queries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, corpus, k1=1.5, b=0.75, epsilon=0.25):\n",
    "        self.corpus_size = len(corpus)\n",
    "        self.avgdl = 0\n",
    "        self.doc_freqs = []\n",
    "        self.idf = {}\n",
    "        self.doc_len = []\n",
    "        \n",
    "        # Calc parameters\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        nd = self._initialize(corpus)\n",
    "        self._calc_idf(nd)\n",
    "\n",
    "\n",
    "    def _initialize(self, corpus):\n",
    "        nd = {}\n",
    "        num_doc = 0\n",
    "        for document in corpus:\n",
    "            self.doc_len.append(len(document))\n",
    "            num_doc += len(document)\n",
    "\n",
    "            frequencies = {}\n",
    "            for word in document:\n",
    "                if word not in frequencies:\n",
    "                    frequencies[word] = 0\n",
    "                frequencies[word] += 1\n",
    "            self.doc_freqs.append(frequencies)\n",
    "\n",
    "            for word, freq in frequencies.items():\n",
    "                if word not in nd:\n",
    "                    nd[word] = 0\n",
    "                nd[word] += 1\n",
    "\n",
    "        self.avgdl = num_doc / self.corpus_size\n",
    "        return nd\n",
    "    \n",
    "    def _calc_idf(self, nd):\n",
    "        # collect idf sum to calculate an average idf for epsilon value\n",
    "        idf_sum = 0\n",
    "        # collect words with negative idf to set them a special epsilon value.\n",
    "        # idf can be negative if word is contained in more than half of documents\n",
    "        negative_idfs = []\n",
    "        for word, freq in nd.items():\n",
    "            idf = math.log(self.corpus_size - freq + 0.5) - math.log(freq + 0.5)\n",
    "            self.idf[word] = idf\n",
    "            idf_sum += idf\n",
    "            if idf < 0:\n",
    "                negative_idfs.append(word)\n",
    "        self.average_idf = idf_sum / len(self.idf)\n",
    "\n",
    "        eps = self.epsilon * self.average_idf\n",
    "        for word in negative_idfs:\n",
    "            self.idf[word] = eps\n",
    "\n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
    "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25(text_corpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = text_queries[0][0]\n",
    "doc_scores = bm25.get_scores(text_queries[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['international', 'organized', 'crime']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_queries[0][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('301', 'FBIS4-41991', 18.50459297775577)\n",
      "('302', 'FBIS4-67701', 37.802274284004476)\n",
      "('303', 'FT921-7107', 35.62332964986995)\n",
      "('304', 'FR940419-2-00009', 31.402554438878067)\n",
      "('305', 'LA112489-0003', 14.380538952839482)\n",
      "('306', 'FT921-13505', 19.769796389716703)\n"
     ]
    }
   ],
   "source": [
    "f = open('data/run.bm25.txt', 'a')\n",
    "for query in text_queries:\n",
    "    topic = query[0]\n",
    "    tokenized_query = query[1:]\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    keys = list(text_corpus.keys())\n",
    "\n",
    "    topic_list = []\n",
    "    for item in range(len(doc_scores)):\n",
    "        topic_list.append((topic,keys[item],doc_scores[item]))\n",
    "    topic_list.sort(key=lambda tup: tup[2], reverse=True)\n",
    "    # filter out topics with relavance score 0\n",
    "    topics_list_filtered = [tup for tup in topic_list if tup[2] != 0]\n",
    "    for item in range(len(topics_list_filtered)):\n",
    "        (topic, docid, score) = topics_list_filtered[item]\n",
    "        print(topic,\" \",\"Q0\",\" \",docid,\" \",item+1,\" \",score,\" \",\"BM25\", file=f)   \n",
    "    print(topics_list_filtered[0])   \n",
    "f.close() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Query BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=text_queries[0][1:],topn=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid aproach (taking average)\n",
    "f = open('data/expanded.centroid.queries.txt', 'a')\n",
    "for query in text_queries:\n",
    "    vectors = np.zeros(300)\n",
    "    for word in query[1:]:\n",
    "        if word in model.vocab:\n",
    "            vectors += model[word]\n",
    "    similar_words = model.similar_by_vector(vectors/len(query[1:]),topn=3)\n",
    "    expansion = \" \".join([ x[0] for x in similar_words ])\n",
    "    print(\" \".join(query),expansion, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_queries = []\n",
    "with open(\"data/expanded.centroid.queries.txt\") as f:\n",
    "    lines = ''.join(f.readlines())\n",
    "expanded_queries = [line.rstrip().split() for line in lines.split('\\n')[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/run.expanded.centroid.bm25.txt', 'a')\n",
    "for query in expanded_queries:\n",
    "    topic = query[0]\n",
    "    tokenized_query = query[1:]\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    keys = list(text_corpus.keys())\n",
    "\n",
    "    topic_list = []\n",
    "    for item in range(len(doc_scores)):\n",
    "        topic_list.append((topic,keys[item],doc_scores[item]))\n",
    "    topic_list.sort(key=lambda tup: tup[2], reverse=True)\n",
    "    # filter out topics with relavance score 0\n",
    "    topics_list_filtered = [tup for tup in topic_list if tup[2] != 0]\n",
    "    for item in range(len(topics_list_filtered)):\n",
    "        (topic, docid, score) = topics_list_filtered[item]\n",
    "        print(topic,\" \",\"Q0\",\" \",docid,\" \",item+1,\" \",score,\" \",\"BM25+QE\", file=f)   \n",
    "    print(topics_list_filtered[0])   \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion aproach (taking average)\n",
    "f = open('data/expanded.fusion.queries.txt', 'a')\n",
    "for query in text_queries:\n",
    "    expanding_words = []\n",
    "    for word in query[1:]:\n",
    "        if word in model.vocab:\n",
    "            expanding_words.append(model.similar_by_word(word,topn=1))\n",
    "    expanding_words = sum(expanding_words, [])\n",
    "    expansion = \" \".join([ x[0] for x in expanding_words ])\n",
    "    print(\" \".join(query),expansion, file=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_queries = []\n",
    "with open(\"data/expanded.fusion.queries.txt\") as f:\n",
    "    lines = ''.join(f.readlines())\n",
    "expanded_queries = [line.rstrip().split() for line in lines.split('\\n')[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/run.expanded.fusion.bm25.txt', 'a')\n",
    "for query in expanded_queries:\n",
    "    topic = query[0]\n",
    "    tokenized_query = query[1:]\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    keys = list(text_corpus.keys())\n",
    "\n",
    "    topic_list = []\n",
    "    for item in range(len(doc_scores)):\n",
    "        topic_list.append((topic,keys[item],doc_scores[item]))\n",
    "    topic_list.sort(key=lambda tup: tup[2], reverse=True)\n",
    "    # filter out topics with relavance score 0\n",
    "    topics_list_filtered = [tup for tup in topic_list if tup[2] != 0]\n",
    "    for item in range(len(topics_list_filtered)):\n",
    "        (topic, docid, score) = topics_list_filtered[item]\n",
    "        print(topic,\" \",\"Q0\",\" \",docid,\" \",item+1,\" \",score,\" \",\"BM25+QE\", file=f)   \n",
    "    print(topics_list_filtered[0])   \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension of BM25 Class, WIP\n",
    "class BM25WE(BM25):\n",
    "    def __init__(self, corpus):\n",
    "        super().__init__(corpus)\n",
    "        \n",
    "    def get_scores(self, query):\n",
    "        score = np.zeros(self.corpus_size)\n",
    "        doc_len = np.array(self.doc_len)\n",
    "        for q in query:\n",
    "            q_freq = np.array([(doc.get(q) or 0) for doc in self.doc_freqs])\n",
    "            score += (self.idf.get(q) or 0) * (q_freq * (self.k1 + 1) /\n",
    "                                               (q_freq + self.k1 * (1 - self.b + self.b * doc_len / self.avgdl)))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
