{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPTW vs TF-IDF\n",
    "This Jupyter Notebook Implements the methods described in the paper Contextually Propogated Term Weights for Document. We compare the results with the popular TF-IDF algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Do (Possibel that this cell should be deleted)\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proccessing Training Data\n",
    "Below the training data is proccesed such that we have the labels for the different training data in the variable **train_labels** and the corresponding text in **train_text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open(\"reuters/r8-train-all-terms.txt\", \"r\")\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "for x in train_file:\n",
    "    split = x.split()\n",
    "    train_labels += [split[0]]\n",
    "    words = \" \".join(split[1:])\n",
    "    train_texts += [words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proccessing Test Data\n",
    "We repeat the same steps with the test data with the respective variables beign **test_labels** and **test_text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open(\"reuters/r8-test-all-terms.txt\", \"r\")\n",
    "test_texts = []\n",
    "test_labels = []\n",
    "for x in test_file:\n",
    "    split = x.split()\n",
    "    test_labels += [split[0]]\n",
    "    words = \" \".join(split[1:])\n",
    "    test_texts += [words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training TF-IDF Model\n",
    "Training a prediction model using TD-IDF as a baseline to compare with the CPTW algorithm that will be implemented later. The code is copied from [stack-overflow](https://stackoverflow.com/questions/43494059/list-of-tfidf-points-for-scikit-nearest-neighbor). Code uses K = 1 for KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "training = train_texts\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(training)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "neigh = NearestNeighbors(n_neighbors=1, n_jobs=-1) \n",
    "neigh.fit(X_train_tfidf)\n",
    "\n",
    "test= test_texts\n",
    "X_test_counts = count_vect.transform(test)\n",
    "\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "comp = neigh.kneighbors(X_test_tfidf, return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline result of TF-IDF\n",
    "Results of KNN on TF- IDF Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.840109639104614\n",
      "0.7862565745939916\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "pred_labels = [train_labels[int(idx)] for idx in comp]\n",
    "print(f1_score(test_labels, pred_labels, average = \"micro\"))\n",
    "print(f1_score(test_labels, pred_labels, average = \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing CPTW\n",
    "In order to implement CPTW, we need to first get the Word Embedding using the word2vec implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    model1 = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        size=150,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=10)\n",
    "    model.train(documents, total_examples=len(documents), epochs=10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15587"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftrain_text = [w for row in train_texts for w in row.split() if w in model.vocab]\n",
    "ftest_text = [w for row in test_texts for w in row.split() if w in model.vocab]\n",
    "\n",
    "X_train = [[w for w in row.split() if w in model.vocab] for row in train_texts]\n",
    "X_test = [[w for w in row.split() if w in model.vocab] for row in test_texts]\n",
    "combined = ftrain_text + ftest_text\n",
    "unique_words = {words for words in (ftrain_text + ftest_text)}\n",
    "words = list(unique_words)\n",
    "len(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cats', 0.8099379539489746), ('dog', 0.7609456777572632), ('kitten', 0.7464985251426697), ('feline', 0.7326233983039856), ('beagle', 0.7150583267211914), ('puppy', 0.7075453996658325), ('pup', 0.6934291124343872), ('pet', 0.6891531348228455), ('felines', 0.6755931377410889), ('chihuahua', 0.6709762215614319)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(model.most_similar(\"cat\"))\n",
    "def restrict_w2v(w2v, restricted_word_set):\n",
    "    new_vectors = []\n",
    "    new_vocab = {}\n",
    "    new_index2entity = []\n",
    "    new_vectors_norm = []\n",
    "\n",
    "    for i in range(len(w2v.vocab)):\n",
    "        word = w2v.index2entity[i]\n",
    "        vec = w2v.vectors[i]\n",
    "        vocab = w2v.vocab[word]\n",
    "        vec_norm = w2v.vectors_norm[i]\n",
    "        if word in restricted_word_set:\n",
    "            vocab.index = len(new_index2entity)\n",
    "            new_index2entity.append(word)\n",
    "            new_vocab[word] = vocab\n",
    "            new_vectors.append(vec)\n",
    "            new_vectors_norm.append(vec_norm)\n",
    "\n",
    "    w2v.vocab = new_vocab\n",
    "    w2v.vectors = np.array(new_vectors)\n",
    "    w2v.index2entity = np.array(new_index2entity)\n",
    "    w2v.index2word = np.array(new_index2entity)\n",
    "    w2v.vectors_norm = np.array(new_vectors_norm)\n",
    "restrict_w2v(model, unique_words)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processed 10 files\n",
      "Currently processed 20 files\n",
      "Currently processed 30 files\n",
      "Currently processed 40 files\n",
      "Currently processed 50 files\n",
      "Currently processed 60 files\n",
      "Currently processed 70 files\n",
      "Currently processed 80 files\n",
      "Currently processed 90 files\n",
      "Currently processed 100 files\n",
      "Currently processed 110 files\n",
      "Currently processed 120 files\n",
      "Currently processed 130 files\n",
      "Currently processed 140 files\n",
      "Currently processed 150 files\n",
      "Currently processed 160 files\n",
      "Currently processed 170 files\n",
      "Currently processed 180 files\n",
      "Currently processed 190 files\n",
      "Currently processed 200 files\n",
      "Currently processed 210 files\n",
      "Currently processed 220 files\n",
      "Currently processed 230 files\n",
      "Currently processed 240 files\n",
      "Currently processed 250 files\n",
      "Currently processed 260 files\n",
      "Currently processed 270 files\n",
      "Currently processed 280 files\n",
      "Currently processed 290 files\n",
      "Currently processed 300 files\n",
      "Currently processed 310 files\n",
      "Currently processed 320 files\n",
      "Currently processed 330 files\n",
      "Currently processed 340 files\n",
      "Currently processed 350 files\n",
      "Currently processed 360 files\n",
      "Currently processed 370 files\n",
      "Currently processed 380 files\n",
      "Currently processed 390 files\n",
      "Currently processed 400 files\n",
      "Currently processed 410 files\n",
      "Currently processed 420 files\n",
      "Currently processed 430 files\n",
      "Currently processed 440 files\n",
      "Currently processed 450 files\n",
      "Currently processed 460 files\n",
      "Currently processed 470 files\n",
      "Currently processed 480 files\n",
      "Currently processed 490 files\n",
      "Currently processed 500 files\n",
      "Currently processed 510 files\n",
      "Currently processed 520 files\n",
      "Currently processed 530 files\n",
      "Currently processed 540 files\n",
      "Currently processed 550 files\n",
      "Currently processed 560 files\n",
      "Currently processed 570 files\n",
      "Currently processed 580 files\n",
      "Currently processed 590 files\n",
      "Currently processed 600 files\n",
      "Currently processed 610 files\n",
      "Currently processed 620 files\n",
      "Currently processed 630 files\n",
      "Currently processed 640 files\n",
      "Currently processed 650 files\n",
      "Currently processed 660 files\n",
      "Currently processed 670 files\n",
      "Currently processed 680 files\n",
      "Currently processed 690 files\n",
      "Currently processed 700 files\n",
      "Currently processed 710 files\n",
      "Currently processed 720 files\n",
      "Currently processed 730 files\n",
      "Currently processed 740 files\n",
      "Currently processed 750 files\n",
      "Currently processed 760 files\n",
      "Currently processed 770 files\n",
      "Currently processed 780 files\n",
      "Currently processed 790 files\n",
      "Currently processed 800 files\n",
      "Currently processed 810 files\n",
      "Currently processed 820 files\n",
      "Currently processed 830 files\n",
      "Currently processed 840 files\n",
      "Currently processed 850 files\n",
      "Currently processed 860 files\n",
      "Currently processed 870 files\n",
      "Currently processed 880 files\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "def gamma(w_k, w_j, d_i, cossim):\n",
    "    freq = d_i.count(w_k)\n",
    "    sim = cossim\n",
    "    return freq * sim\n",
    "\n",
    "def cptw(d_i, tau = 5):\n",
    "    result = np.zeros(len(unique_words))\n",
    "    for idx in range(len(words)):\n",
    "        if (idx % 1000 == 0):        w_j = words[idx]\n",
    "        most_similar = [(w_j, 1.0)] + model.similar_by_word(w_j, topn = 5)\n",
    "\n",
    "\n",
    "        ws = [w for (w, c) in most_similar]\n",
    "        cs = [c for (w, c) in most_similar]\n",
    "\n",
    "        alpha_j = 1 / sum(cs)\n",
    "        gammas = [gamma(w_k, w_j, d_i, c_k) for (w_k , c_k) in most_similar]\n",
    "        result[idx] = alpha_j * sum(gammas)\n",
    "    return result\n",
    "\n",
    "TrData = []\n",
    "count = 0\n",
    "for f in ftrain_text:\n",
    "    TrData += [cptw(f)]\n",
    "    count += 1\n",
    "    if count % 10 == 0:\n",
    "        print(\"Currently processed \" + str(count) + \" files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1920929e-07"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
